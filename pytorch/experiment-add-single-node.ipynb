{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructive Neural Network Tutorial 1\n",
    "A constructive algorithm can create new nodes and connections in artificial neural networks (ANNs) during training or operation.\n",
    "Constructive algorithms gained some attention shortly after neural networks with hidden layers were found to increase learning capabilities.\n",
    "The number of neurons in hidden layers of the network is an important factor in the ANN performance and is usually determined by a trial-and-error cycle of manually defining the network size then training and evaluating it.\n",
    "\n",
    "Popular neural network libraries such as TensorFlow and PyTorch do not natively have methods for changing the size of the data structure that store connection weights or other network parameters.\n",
    "This tutorial will start by introducing the fundamental features of a constructive algorithm neural network, then discuss some approaches to increasing the size of a neural network in PyTorch during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamental algorithm components\n",
    "Constructive algorithms are intended to improve the performance of an artificial neural network by adding neurons or connections.\n",
    "Designing a constructive algorithm requires a number of important decisions to be made:\n",
    " * When to add neurons (or connections)\n",
    " * Where to add the neurons (input and output connections)\n",
    " * What parameter values to assign new network components\n",
    " \n",
    "The implementation of a constructive algorithm is another important design consideration, but algorithms may be defined theoretically before practical implementation.\n",
    "\n",
    "Examples of predefined constructive algorithm processes include:\n",
    " * Adding neurons at a fixed rate during training\n",
    " * Selecting a specific layer to add all neurons\n",
    " * Assigning parameters (e.g., weights) with predefined or random values\n",
    "\n",
    "Constructive algorithms may also make these decisions during operation, typically by considering the ANN performance, for example:\n",
    " * Neurons may be constructed if the decrease in training error has slowed or stopped\n",
    " * Performance of individual neurons or layers may indicate a location to perform construction\n",
    " * Parameters for new neuron may be calculated from network inputs\n",
    "\n",
    "Dynamic Node Creation (DNC) is an early constructive algorithm developed by Timur Ash in 1989. \n",
    "DNC calculates a rate of training error descent and adds a neuron to the hidden layer if the decrease in error slows.\n",
    "\n",
    "This tutorial will use the rate of validation error descent to trigger construction, create new neurons in a preselected hidden layer of a feedforward network, and provide randomly initialised weights for new neurons.\n",
    "This tutorial will discuss and demonstrate methods for adding neurons to a network in PyTorch.\n",
    "The generality of these methods should allow implementations in TensorFlow and other ANN libraries.\n",
    "\n",
    "Later tutorials will investigate the application of constructive algorithm for deep networks, visual object recognition, transfer learning, and retraining without forgetting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding neurons in PyTorch\n",
    "Changing the number of weights in an ANN is complicated by them being stored in Tensors objects that do not include a method for changing the size.\n",
    "Two options that exist for adding new neurons and connection weights are:\n",
    "1. Creating a new larger Tensor, copying the previous parameters and adding the new ones.\n",
    "2. Starting with a Tensor larger than the number of neurons and adding neurons to the unused space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "iris_data = list()\n",
    "iris_type = list()\n",
    "with open('iris.data') as f:\n",
    "    iris_reader = csv.reader(f)\n",
    "    for csv_row in iris_reader:\n",
    "        if len(csv_row) > 0:\n",
    "            iris_row = []\n",
    "            for n in csv_row[:-1]:\n",
    "                iris_row.append(float(n))\n",
    "            iris_data.append(iris_row)\n",
    "            if csv_row[-1] == 'Iris-setosa':\n",
    "                iris_type.append(0)\n",
    "            elif csv_row[-1] == 'Iris-versicolor':\n",
    "                iris_type.append(1)\n",
    "            elif csv_row[-1] == 'Iris-virginica':\n",
    "                iris_type.append(2)\n",
    "            else:\n",
    "                iris_type.append(-1)\n",
    "        \n",
    "#print(iris_data)\n",
    "#print(iris_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def add_np_randn_rows(new, arr):\n",
    "    return np.append(arr,np.random.randn(new,arr.shape[1]),0)\n",
    "\n",
    "def add_np_randn_cols(new, arr):\n",
    "    return np.append(arr,np.random.randn(arr.shape[0],new),1)\n",
    "\n",
    "#w_ji = np.random.randn(3,3)\n",
    "#print(w_ji)\n",
    "\n",
    "#w_ji = add_np_randn_cols(1,w_ji)\n",
    "#print(w_ji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.1, 3.5, 1.4, 0.2]\n",
      "[0.03048774 0.01032446]\n",
      "[0.32434655 0.33447449 0.34117897]\n"
     ]
    }
   ],
   "source": [
    "def nn_sigmoid(w_in, bias, a_in):\n",
    "    a_out = 1 / (1 + np.exp(- (w_in @ a_in + bias)))\n",
    "    return a_out\n",
    "    \n",
    "def nn_softmax(w_in, a_in):\n",
    "    a_out = np.exp( w_in @ (a_in - np.amax(a_in)) )  # reduce likelihood of NaN values \n",
    "    a_out = a_out / np.sum(a_out) \n",
    "    return a_out\n",
    "    \n",
    "def nn_add_neurons(new, j, w_ji, bias_j, w_kj):\n",
    "    j += new\n",
    "    w_ji = add_np_randn_rows(new,w_ji)\n",
    "    bias_j = add_np_randn_rows(new,bias_j)\n",
    "    w_kj = add_np_randn_cols(new,w_kj)\n",
    "    return j, w_ji, bias_j, w_kj\n",
    "    \n",
    "# create an artificial neural network\n",
    "input_neurons = 4\n",
    "hidden_neurons = 2\n",
    "output_neurons = 3\n",
    "\n",
    "w_hi = np.random.randn(hidden_neurons, input_neurons)\n",
    "bias_h = np.random.randn(hidden_neurons)\n",
    "\n",
    "w_oh = np.random.randn(output_neurons, hidden_neurons)\n",
    "\n",
    "activations_i = iris_data[0][:]\n",
    "print(activations_i)\n",
    "\n",
    "activations_h = nn_sigmoid(w_hi, bias_h, activations_i)\n",
    "print(activations_h)\n",
    "\n",
    "activations_o = nn_softmax(w_oh, activations_h)\n",
    "print(activations_o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3423919304971338\n",
      "[-1.78211736e-05 -8.35050736e-06  4.35106020e-06 -3.50143548e-08]\n"
     ]
    }
   ],
   "source": [
    "def error_sq(label, nn_out):\n",
    "    t = np.zeros(nn_out.shape)\n",
    "    t[label] = 1.0\n",
    "    error = 0.5 * np.sum(np.square(t - nn_out))\n",
    "    return error\n",
    "\n",
    "error = error_sq(iris_type[0], activations_o)\n",
    "print(error)\n",
    "\n",
    "#print(w_oh)\n",
    "#print(w_oh.transpose())\n",
    "\n",
    "# Backpropagation\n",
    "d_output = (activations_o - error) * activations_o * (1 - activations_o)\n",
    "d_hidden = (w_oh.transpose() @ d_output) * activations_h * (1 - activations_h)\n",
    "d_input = (w_hi.transpose() @ d_hidden) * activations_i\n",
    "\n",
    "#print(d_output)\n",
    "#print(d_hidden)\n",
    "#print(d_input)\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "#print(d_input.shape)\n",
    "#print(activations_h[np.newaxis].transpose())\n",
    "\n",
    "delta_w_hi = - learning_rate * activations_h[np.newaxis].transpose() @ d_input[np.newaxis]\n",
    "#print(delta_w_hi)\n",
    "delta_b_h = - learning_rate * d_input\n",
    "print(delta_b_h)\n",
    "\n",
    "delta_w_oh = - learning_rate * activations_o[np.newaxis].transpose() @ d_hidden[np.newaxis]\n",
    "#print(delta_w_oh)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DynamicNodeCreation(\n",
      "  (fcs): ModuleList(\n",
      "    (0): Linear(in_features=4, out_features=1, bias=True)\n",
      "    (1): Linear(in_features=1, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# modified from code posted: \n",
    "# https://discuss.pytorch.org/t/possible-to-add-initialize-new-nodes-to-hidden-layer-partway-through-training/3809/4  \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DynamicNodeCreation(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DynamicNodeCreation, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "          \n",
    "        # initialize weights\n",
    "        self.fcs = nn.ModuleList([nn.Linear(self.input_size, self.hidden_size)])\n",
    "        self.fcs.append(nn.Linear(self.hidden_size, self.output_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # hidden sigmoid layer\n",
    "        x = F.sigmoid(self.fcs[0](x))\n",
    "        x = F.log_softmax(self.fcs[1](x))\n",
    "\n",
    "        \n",
    "    def add_units(self, n_new):\n",
    "        # take a copy of the current weights stored in self.fcs\n",
    "        current = [ix.weight.data for ix in self.fcs]\n",
    "\n",
    "        # make the new weights in and out of hidden layer you are adding neurons to\n",
    "        hl_input = torch.zeros([n_new, current[0].shape[1]])\n",
    "        nn.init.xavier_uniform_(hl_input, gain=nn.init.calculate_gain('sigmoid'))\n",
    "        hl_output = torch.zeros([current[1].shape[0], n_new])\n",
    "        nn.init.xavier_uniform_(hl_input)\n",
    "\n",
    "        # concatenate the old weights with the new weights\n",
    "        new_wi = torch.cat([current[0], hl_input], dim=0)\n",
    "        new_wo = torch.cat([current[1], hl_output], dim=1)\n",
    "\n",
    "        # reset weight and grad variables to new size\n",
    "        self.fcs[0] = nn.Linear(current[0].shape[1], self.hidden_size)\n",
    "        self.fcs[1] = nn.Linear(self.hidden_size, current[1].shape[0])\n",
    "\n",
    "        # set the weight data to new values\n",
    "        self.fcs[0].weight.data = torch.tensor(new_wi, requires_grad=True)\n",
    "        self.fcs[1].weight.data = torch.tensor(new_wo, requires_grad=True)\n",
    "\n",
    "net = DynamicNodeCreation(4,1,3)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(params[0].size())\n",
    "\n",
    "net.add_units(1)\n",
    "params = list(net.parameters())\n",
    "print(params[0].size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
