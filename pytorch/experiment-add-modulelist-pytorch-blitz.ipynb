{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructive Neural Networks Tutorial\n",
    "This tutorial will demonstrate an approach to implementing a constructive algorithm in PyTorch.\n",
    "Modifying a tutorial provided in the 60-minute Blitz on the PyTorch website: https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[2].size())  # conv2's .weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Split: train\n",
      "    Root Location: MNIST\n",
      "    Transforms (if any): Compose(\n",
      "                             Pad(padding=2, fill=0, padding_mode=constant)\n",
      "                             ToTensor()\n",
      "                             Normalize(mean=(0.1307,), std=(0.3081,))\n",
      "                         )\n",
      "    Target Transforms (if any): None, 'batch_size': 4, 'num_workers': 0, 'collate_fn': <function default_collate at 0x0000020C191E9F28>, 'pin_memory': False, 'drop_last': False, 'timeout': 0, 'worker_init_fn': None, 'sampler': <torch.utils.data.sampler.RandomSampler object at 0x0000020C1D4E2940>, 'batch_sampler': <torch.utils.data.sampler.BatchSampler object at 0x0000020C1D4E2A20>, '_DataLoader__initialized': True}\n",
      "(tensor([[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         ...,\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]), tensor(5))\n"
     ]
    }
   ],
   "source": [
    "from TLDatasetLoaders import MNISTDataset\n",
    "\n",
    "m_data = MNISTDataset()\n",
    "\n",
    "print(vars(m_data.trainloader))\n",
    "print(m_data.trainset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2) tensor(2) tensor(7) tensor(4)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from TLDatasetLoaders import imshow\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(m_data.testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "# print labels\n",
    "print(' '.join('%5s' % labels[j] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimiser\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(m_data.trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(m_data.testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % labels[j] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(images)\n",
    "\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % predicted[j]\n",
    "                              for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in m_data.testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in m_data.testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        i, 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConstructiveNet(\n",
      "  (layers): ModuleList(\n",
      "    (0): ModuleList(\n",
      "      (0): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "      (1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "    )\n",
      "    (1): ModuleList(\n",
      "      (0): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    )\n",
      "    (2): ModuleList(\n",
      "      (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "    )\n",
      "    (3): ModuleList(\n",
      "      (0): Linear(in_features=120, out_features=10, bias=True)\n",
      "    )\n",
      "    (4): ModuleList(\n",
      "      (0): Linear(in_features=10, out_features=10, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConstructiveNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ConstructiveNet, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.ModuleList([nn.Conv2d(1,3,5), nn.Conv2d(1,3,5)]))\n",
    "        self.layers.append(nn.ModuleList([nn.Conv2d(6,16,5)]))\n",
    "        self.layers.append(nn.ModuleList([nn.Linear(16 * 5 * 5, 120)]))\n",
    "        self.layers.append(nn.ModuleList([nn.Linear(120, 10)]))\n",
    "        self.layers.append(nn.ModuleList([nn.Linear(10, 10)]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(torch.cat([net_m(x) for net_m in iter(self.layers[0])],1)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.layers[1][0](x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.layers[2][0](x))\n",
    "        x = F.relu(self.layers[3][0](x))\n",
    "        x = self.layers[4][0](x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "    def append_kaiming_uniform(self, weight_parameter, new_channels, \n",
    "                                a=0, mode='fan_in', \n",
    "                                nonlinearity='leaky_relu'):\n",
    "        size = [m for m in weight_parameter.size()]\n",
    "        size_append = [m for m in weight_parameter.size()]\n",
    "        size[1] += new_channels \n",
    "        size_append[1] = new_channels\n",
    "        \n",
    "        # calculate fan_in or fan_out\n",
    "        if len(size)<2:\n",
    "            raise ValueError(\"Fan in and fan out can not be computed for tensor with fewer than 2 dimensions\")\n",
    "        fan_x = size[1] if mode=='fan_in' else size[0]\n",
    "        if len(size)>2:\n",
    "            #num_input_fmaps = size[1]\n",
    "            #num_output_fmaps = size[0]\n",
    "            #receptive_field_size = tensor[0][0].numel()\n",
    "            fan_x *= weight_parameter[0][0].numel()\n",
    "        gain = torch.nn.init.calculate_gain(nonlinearity, a)\n",
    "        std = gain / math.sqrt(fan_x)\n",
    "        bound = math.sqrt(3.0) * std\n",
    "        \n",
    "        weight_tensor_append = torch.empty(size_append, requires_grad=True)\n",
    "        with torch.no_grad():\n",
    "            weight_tensor_append.data.uniform_(-bound,bound)\n",
    "        return torch.cat([weight_parameter.data, weight_tensor_append],1)\n",
    "    \n",
    "    def new_in_channels(self, nmod, new_channels):\n",
    "        nmod_size = nmod.weight.size()\n",
    "        nmod.in_channels += new_channels\n",
    "        nmod.weight.data = self.append_kaiming_uniform(nmod.weight, new_channels)\n",
    "    \n",
    "    # append a new nn.Module (m) to the network layer (layer)\n",
    "    def append_layer_module(self, layer, m):\n",
    "        if layer < len(self.layers):\n",
    "            self.layers[layer].append(m)\n",
    "            if layer < len(self.layers)-1:\n",
    "                for nmod in iter(self.layers[layer+1]):\n",
    "                    self.new_in_channels(nmod, m.out_channels)\n",
    "\n",
    "constructive_net = ConstructiveNet()\n",
    "print(constructive_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 6, 5, 5])\n",
      "ConstructiveNet(\n",
      "  (layers): ModuleList(\n",
      "    (0): ModuleList(\n",
      "      (0): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "      (1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "      (2): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "    )\n",
      "    (1): ModuleList(\n",
      "      (0): Conv2d(9, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    )\n",
      "    (2): ModuleList(\n",
      "      (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "    )\n",
      "    (3): ModuleList(\n",
      "      (0): Linear(in_features=120, out_features=10, bias=True)\n",
      "    )\n",
      "    (4): ModuleList(\n",
      "      (0): Linear(in_features=10, out_features=10, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "torch.Size([16, 9, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "#net_input = torch.randn(1,1,32,32)\n",
    "\n",
    "#constructive_net.layers[0][1].weight\n",
    "#torch.cat([constructive_net.layers[0][i](net_input) for i in enumerate(constructive_net.layers[0])])\n",
    "#F.relu(constructive_net.layers[0][0](torch.randn(1,1,32,32)))\n",
    "\n",
    "#torch.cat([net_m(net_input) for nnn in iter(constructive_net.layers[0])],1).size()\n",
    "\n",
    "#import matplotlib.pyplot as plt \n",
    "#plt.hist(constructive_net.layers[1][0].weight.data.flatten(),25)\n",
    "#plt.hist((torch.randn((1,3000)))/15,25)\n",
    "\n",
    "# things to change in a conv module following a layer that increases size:\n",
    "#  - self.in_channels\n",
    "#  - self.weight\n",
    "\n",
    "#print(len(constructive_net.layers))\n",
    "#print(constructive_net.layers[0][0].weight.size()) #out_channels\n",
    "#print(torch.zeros((3,1,3)))\n",
    "#print(constructive_net.layers[0][0].weight)\n",
    "#t_new = constructive_net.layers[0][0].weight\n",
    "#t_new[0,0,0,0] = 1\n",
    "\n",
    "#new_channels = 2\n",
    "#w_size = constructive_net.layers[0][0].weight.size()\n",
    "#w_size = [m for m in constructive_net.layers[0][0].weight.size()]\n",
    "\n",
    "#w_size[0] = new_channels\n",
    "#w_new = torch.zeros(w_size, requires_grad=True)\n",
    "\n",
    "#reproduce kaiming_uniform_\n",
    "#receptive_field_size = constructive_net.layers[0][0].weight[0][0].numel()\n",
    "\n",
    "#print(receptive_field_size)\n",
    "\n",
    "#print(constructive_net.layers[0][0].in_channels)\n",
    "\n",
    "#print(constructive_net.layers[1][0].weight)\n",
    "#print(constructive_net.layers[1][0].weight.data)\n",
    "\n",
    "print(constructive_net.layers[1][0].weight.size())\n",
    "constructive_net.append_layer_module(0, nn.Conv2d(1,3,5))\n",
    "print(constructive_net)\n",
    "print(constructive_net.layers[1][0].weight.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimiser\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "cn_criterion = nn.CrossEntropyLoss()\n",
    "cn_optimizer = optim.SGD(constructive_net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 0.722\n",
      "[1,  4000] loss: 0.192\n",
      "[1,  6000] loss: 0.152\n",
      "[1,  8000] loss: 0.125\n",
      "[1, 10000] loss: 0.094\n",
      "[1, 12000] loss: 0.096\n",
      "[1, 14000] loss: 0.076\n",
      "[2,  2000] loss: 0.074\n",
      "[2,  4000] loss: 0.065\n",
      "[2,  6000] loss: 0.057\n",
      "[2,  8000] loss: 0.058\n",
      "[2, 10000] loss: 0.059\n",
      "[2, 12000] loss: 0.053\n",
      "[2, 14000] loss: 0.051\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(m_data.trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        cn_optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = constructive_net(inputs)\n",
    "        loss = cn_criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        cn_optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundTruth:  tensor(8) tensor(2) tensor(2) tensor(3)\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(m_data.testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % labels[j] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  tensor(8) tensor(2) tensor(2) tensor(3)\n"
     ]
    }
   ],
   "source": [
    "outputs = constructive_net(images)\n",
    "\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % predicted[j]\n",
    "                              for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in m_data.testloader:\n",
    "        images, labels = data\n",
    "        outputs = constructive_net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 98.72 %\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of the network on the 10000 test images: %.2f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in m_data.testloader:\n",
    "        images, labels = data\n",
    "        outputs = constructive_net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of    0 :    970 /  980 :   98.98 %\n",
      "Accuracy of    1 :   1125 / 1135 :   99.12 %\n",
      "Accuracy of    2 :   1019 / 1032 :   98.74 %\n",
      "Accuracy of    3 :   1006 / 1010 :   99.60 %\n",
      "Accuracy of    4 :    968 /  982 :   98.57 %\n",
      "Accuracy of    5 :    880 /  892 :   98.65 %\n",
      "Accuracy of    6 :    951 /  958 :   99.27 %\n",
      "Accuracy of    7 :   1014 / 1028 :   98.64 %\n",
      "Accuracy of    8 :    958 /  974 :   98.36 %\n",
      "Accuracy of    9 :    981 / 1009 :   97.22 %\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print('Accuracy of %4s : %6d / %4d :   %.2f %%' % (\n",
    "        i, class_correct[i], class_total[i], 100 * class_correct[i] / class_total[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
